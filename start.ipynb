{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## base model下载以及环境配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('msmarco-distilbert-base-tas-b',cache_folder='sbert/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install pandas\n",
    "! pip install matplotlib\n",
    "! pip install openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 含query log的数据集构造"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 从pkl文件中加载query数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "267\n",
      "111\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import json\n",
    "file_folder = 'bert-sql/pkl/'\n",
    "\n",
    "datasets =\"synthea\"\n",
    "## 读入pkl文件\n",
    "with open(file_folder+\"omop.pkl\", \"rb\") as f:\n",
    "    query1 = pickle.load(f)\n",
    "\n",
    "with open(file_folder+\"{}.pkl\".format(datasets), \"rb\") as f:\n",
    "    query2 = pickle.load(f)\n",
    "\n",
    "print(len(query1))\n",
    "print(len(query2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 合并schema数据和query数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_filefolder = f\"datasets/{datasets}/\"\n",
    "columns = len(query2)\n",
    "\n",
    "csvFilePath = data_filefolder+\"omop_{}_data.xlsx\".format(datasets)\n",
    "jsonFilePath = data_filefolder+\"omop_{}.json\".format(datasets)\n",
    "\n",
    "df = pd.read_excel(csvFilePath)\n",
    "\n",
    "jsonString = df.to_json(orient=\"records\", indent=4)\n",
    "\n",
    "## 将csv格式转成json格式\n",
    "with open(jsonFilePath, \"w\") as jsonFile:\n",
    "    jsonFile.write(jsonString)\n",
    "\n",
    "## 写入sql查询信息\n",
    "with open(jsonFilePath) as jsonFile:\n",
    "    datas = json.load(jsonFile)\n",
    "\n",
    "\n",
    "for i in range(len(datas)):\n",
    "    idA = i // columns\n",
    "    idB = i % columns\n",
    "    \n",
    "    del datas[i][\"d1\"]\n",
    "    del datas[i][\"d2\"]\n",
    "    del datas[i][\"d3\"]\n",
    "    del datas[i][\"d4\"]\n",
    "    try:\n",
    "        datas[i][\"queryA\"] = query1[idA]\n",
    "        datas[i][\"queryB\"] = query2[idB]\n",
    "    except:\n",
    "        print(i)\n",
    "        print(idA,idB)\n",
    "        break\n",
    "\n",
    "with open(data_filefolder+\"final.json\".format(datasets), \"w\") as jsonFile:\n",
    "    jsonString = json.dumps(datas, indent=4)\n",
    "    jsonFile.write(jsonString)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 划分train/val/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## random split\n",
    "import random \n",
    "import json\n",
    "\n",
    "jsonFilePath = data_filefolder + \"/final.json\"\n",
    "\n",
    "with open(jsonFilePath) as jsonFile:\n",
    "    datas = json.load(jsonFile)\n",
    "\n",
    "def data_split(full_list, ratio, shuffle=False):\n",
    "    \"\"\"\n",
    "    数据集拆分: 将列表full_list按比例ratio（随机）划分为2个子列表sublist_1与sublist_2\n",
    "    :param full_list: 数据列表\n",
    "    :param ratio:     子列表1\n",
    "    :param shuffle:   子列表2\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    n_total = len(full_list)\n",
    "    offset = int(n_total * ratio)\n",
    "    if n_total == 0 or offset < 1:\n",
    "        return [], full_list\n",
    "    if shuffle:\n",
    "        random.shuffle(full_list)\n",
    "    sublist_1 = full_list[:offset]\n",
    "    sublist_2 = full_list[offset:]\n",
    "    return sublist_1, sublist_2\n",
    "\n",
    "random.seed(5199)\n",
    "test, others = data_split(datas, ratio=0.1, shuffle=True)\n",
    "val,train = data_split(others, ratio=0.1/0.9, shuffle=True)\n",
    "\n",
    "with open(data_filefolder + \"/test0.json\", \"w\") as jsonFile:\n",
    "    jsonString = json.dumps(test, indent=4)\n",
    "    jsonFile.write(jsonString)\n",
    "\n",
    "with open(data_filefolder + \"/val0.json\", \"w\") as jsonFile:\n",
    "    jsonString = json.dumps(val, indent=4)\n",
    "    jsonFile.write(jsonString)\n",
    "\n",
    "with open(data_filefolder + \"/train0.json\", \"w\") as jsonFile:\n",
    "    jsonString = json.dumps(train, indent=4)\n",
    "    jsonFile.write(jsonString)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
